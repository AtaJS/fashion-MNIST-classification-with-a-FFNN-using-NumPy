# Fashion MNIST Classification with a FFNN using NumPy

This project demonstrates the implementation of a feedforward neural network from scratch using NumPy. It's designed as a step-by-step guide to understanding the core concepts of neural networks, including:

* **Forward and Backward Propagation**: Implementing the fundamental algorithms for information flow and gradient calculation in neural networks.
* **Activation Functions**: Utilizing ReLU, Tanh, and Softmax activation functions and exploring their impact.
* **Loss Function**: Implementing the cross-entropy loss function to measure the model's prediction error.
* **Optimization**: Employing mini-batch gradient descent with momentum for efficient model training.
* **Regularization**: Incorporating weight decay (L2 regularization) to prevent overfitting and improve generalization.
* **Hyperparameter Tuning**: Demonstrating basic techniques to optimize network performance through hyperparameter adjustments.

The network is trained and evaluated on the Fashion-MNIST dataset, a popular dataset for image classification. This project provides a hands-on approach to learning the inner workings of neural networks without relying on high-level deep learning libraries, offering a deeper understanding of the underlying mechanisms.

**Author:** Ata Jodeiri Seyedian

**Emails:** ata.jodeiri@student.oulu.fi, sataseyedian@gmail.com
